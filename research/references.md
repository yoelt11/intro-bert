# Useful links

* [BERT inroduction](https://huggingface.co/bert-base-uncased)
* [BERT visualization](https://huggingface.co/exbert/?model=bert-base-uncased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)
* [BERT repository](https://github.com/dbmdz/berts)
* [Encoder Decoder Models](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)
* [Bert generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)

# Research papers

* [Googleâ€™s Neural Machine Translation System: Bridging the Gap
between Human and Machine Translation](https://arxiv.org/pdf/1609.08144v2.pdf)
* [BertGen: Multi-task Generation through BERT](https://arxiv.org/pdf/2106.03484.pdf)
* [Pre-trained Language Models for Text Generation: A Survey](https://arxiv.org/pdf/2201.05273.pdf)
* [Deep Fusing Pre-trained Models into Neural Machine Translation](https://ojs.aaai.org/index.php/AAAI/article/download/21399/21148)
* [BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
* [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345)
* [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/pdf/1907.12461.pdf)

# Git Repositories

* [x-transformers](https://github.com/lucidrains/x-transformers)
* [Masked language modeling](https://github.com/lucidrains/mlm-pytorch)

# Datasets
* [DiscoFuse](https://github.com/google-research-datasets/discofuse)

# Metrics
* [Rouge Metric](https://huggingface.co/spaces/evaluate-metric/rouge)
