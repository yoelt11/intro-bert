{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7EbknQRPUGzNAF+3NvRFG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoelt11/intro-bert/blob/main/intro_bert_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "AcRZKvbjdKNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers[torch]==4.26.1\n",
        "!pip install torch==1.13.1\n",
        "!pip install rouge_score==0.1.2"
      ],
      "metadata": {
        "id": "ey1PUX-hdOKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import libraries"
      ],
      "metadata": {
        "id": "W20L5NNveOau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "from transformers import BertGenerationConfig, BertGenerationDecoder, BertGenerationEncoder, EncoderDecoderModel, BertTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import yaml"
      ],
      "metadata": {
        "id": "iNt-ZHm5eRR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create dataset class"
      ],
      "metadata": {
        "id": "r_MYcvtmfPof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextFuseDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_root):\n",
        "        # -- open dataset\n",
        "        with open(data_root + \"dataset_example.yaml\", \"r\") as stream:\n",
        "            samples = yaml.safe_load(stream)\n",
        "        self.samples = samples\n",
        "        self.length = len(self.samples)\n",
        "        self.encoder_max_length = 512\n",
        "        self.decoder_max_length = 128\n",
        "        # -- initialize tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # -- Tokenize Input\n",
        "        inputs = self.tokenizer(self.samples[idx]['long_text'] , return_tensors='pt', padding='max_length', truncation=True, max_length=self.encoder_max_length)\n",
        "        outputs = self.tokenizer(self.samples[idx]['short_text'], return_tensors='pt', padding='max_length', truncation=True, max_length=self.decoder_max_length)\n",
        "        # -- Create input and label dictionaries\n",
        "        batch = {}\n",
        "        batch[\"input_ids\"] = inputs.input_ids\n",
        "        batch[\"attention_mask\"] = inputs.attention_mask\n",
        "   \n",
        "        batch[\"decoder_input_ids\"] = outputs.input_ids\n",
        "        batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "        batch[\"labels\"] = outputs.input_ids.clone()\n",
        "        batch['labels'][batch['labels'] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return batch "
      ],
      "metadata": {
        "id": "2YM2WMmkfUAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Script"
      ],
      "metadata": {
        "id": "cef6DUMQfffe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "0g_HxUD6fm7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_dataset(batch_size, root_path):\n",
        "    # -- create dataset instance\n",
        "    dataset = TextFuseDataset(root_path)\n",
        "    # -- get train-test size\n",
        "    train_size = int(len(dataset) * 0.8)\n",
        "    test_size = len(dataset) - train_size\n",
        "    # -- use random-split to split data into train and test subsets\n",
        "    train_dataset , test_dataset = random_split(dataset, [train_size, test_size])\n",
        "    # -- create dataloaders for train and test datasets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "    return train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "4kKLWYVafk7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Tokenizer"
      ],
      "metadata": {
        "id": "Hbhs8mumf0Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenizer():\n",
        "    # -- loading Tokenizer \n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", model_max_length=512)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "NHOLHr1ef2Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model"
      ],
      "metadata": {
        "id": "c0c6sS5of5z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "\n",
        "    # -- initializing (pretrained) Encoder\n",
        "    encoder = BertGenerationEncoder.from_pretrained(\"bert-base-uncased\", \n",
        "            bos_token_id=tokenizer.cls_token_id, \n",
        "            eos_token_id=tokenizer.sep_token_id) \n",
        "\n",
        "    # -- initializing (untrained) decoder BertGeneration config\n",
        "    config = BertGenerationConfig(bos_token_id=tokenizer.cls_token_id, eos_token_id=tokenizer.sep_token_id)\n",
        "    config.is_decoder=True\n",
        "    config.add_cross_attention=True\n",
        "    \n",
        "    # -- initializing a model (untrained) decoder from the config\n",
        "    decoder = BertGenerationDecoder(config)\n",
        "\n",
        "    # -- combining models into a EncoderDecoderModel (transformers package)\n",
        "    encoder_decoder = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
        "    encoder_decoder.config.decoder_start_token_id = tokenizer.cls_token_id \n",
        "    encoder_decoder.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    return encoder_decoder"
      ],
      "metadata": {
        "id": "QFwWG8Jxf8LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Function"
      ],
      "metadata": {
        "id": "tBIobrNff_tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate():\n",
        "    # -- set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # -- metric var\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_fmeasure = 0\n",
        "\n",
        "    for i, batch_data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        # -- get inputs and ouputs\n",
        "        input_ids = batch_data['input_ids'].squeeze(1)\n",
        "        labels = batch_data['labels'].squeeze(1)\n",
        "        labels[labels == -100] = tokenizer.pad_token_id\n",
        "\n",
        "        # -- run inference\n",
        "        pred_ids = model.generate(input_ids=input_ids)\n",
        "\n",
        "        # -- decode strings\n",
        "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "        label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # ?? rouge score does not allow for batched operations?\n",
        "        def get_scores(pred_str, label_str):\n",
        "            precision = 0\n",
        "            recall = 0\n",
        "            fmeasure = 0\n",
        "\n",
        "            for i in range(len(pred_str)):\n",
        "                scores = scorer.score(pred_str[i], label_str[i])\n",
        "                precision += scores['rouge2'].precision\n",
        "                recall += scores['rouge2'].recall\n",
        "                fmeasure += scores['rouge2'].fmeasure\n",
        "            \n",
        "            return precision / i, recall / i, fmeasure / i\n",
        "\n",
        "        precision, recall, fmeasure = get_scores(pred_str, label_str)\n",
        "\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_fmeasure += fmeasure\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            print(f'batched precision: {precision}')\n",
        "            print(f'batched recall: {recall}')\n",
        "            print(f'batched fmeasure: {fmeasure}')\n",
        "\n",
        "            # -- print results\n",
        "            print(pred_str)\n",
        "            print(label_str)\n",
        "\n",
        "    print(f'[Epoch Summary: {epoch + 1}]') \n",
        "    print(f'precision: {epoch_precision / BATCH_SIZE:.3f}')\n",
        "    print(f'recall: {epoch_recall / BATCH_SIZE:.3f}')\n",
        "    print(f'fmeasure: {epoch_fmeasure / BATCH_SIZE:.3f}')"
      ],
      "metadata": {
        "id": "ApZ9JR3ggB7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Function"
      ],
      "metadata": {
        "id": "sEDEodzEgF8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # -- set model training mode\n",
        "    model.train()\n",
        "\n",
        "    # -- initialize running_loss for metrics\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    # -- lock encoder's parameters\n",
        "\n",
        "    for param in model.encoder.parameters(): # only to calculate gradients for decoder\n",
        "        param.requires_grad = False \n",
        "\n",
        "    for i, batch_data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        # -- get inputs and ouputs\n",
        "        input_ids = batch_data['input_ids'].squeeze(1)\n",
        "        labels = batch_data['labels'].squeeze(1)\n",
        "        \n",
        "        # -- set gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # -- run model\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        \n",
        "        # -- calculate losses\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        \n",
        "        # -- perform optimizer step\n",
        "        optimizer.step()\n",
        "        \n",
        "        # -- in-training metrics\n",
        "        running_loss += loss.item()\n",
        "        if i % 20 == 0:\n",
        "            print(f'[Epoch: {epoch + 1}, iteration: {i + 1:5d}] \\nloss: {loss.item() / 100:.3f}')\n",
        "\n",
        "    print(f'[Epoch: {epoch + 1}] \\nloss: {running_loss / BATCH_SIZE:.3f}')"
      ],
      "metadata": {
        "id": "A0vEj3ZjgID0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Training"
      ],
      "metadata": {
        "id": "osaSk7ZmgPpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    # -- set constants\n",
        "    SAVE_DIR = '/tmp/model.pt'\n",
        "    ROOT_PATH = \"./dataset/\"\n",
        "    BATCH_SIZE = 2\n",
        "    LR = 1e-3\n",
        "    WD = 25e-2\n",
        "    EPOCHS = 100\n",
        "    # -- set training device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # -- load dataset\n",
        "    train_dataloader, test_dataloader = load_dataset(BATCH_SIZE, ROOT_PATH)\n",
        "\n",
        "    # -- load tokenizer\n",
        "    tokenizer = load_tokenizer()\n",
        "\n",
        "    # -- load model\n",
        "    model = load_model()\n",
        "\n",
        "    # -- load optimzer\n",
        "    optimizer = optim.AdamW(model.decoder.parameters(),  # optimizing decoder parameters only\n",
        "            lr=LR,\n",
        "            weight_decay=WD)\n",
        "\n",
        "    # -- load metrics\n",
        "    # rouge-n (n-gram) scoring\n",
        "    # rouge-l (longest common subsqeuence) scoring\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
        "\n",
        "    # --  training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"----------------- EPOCH: {epoch}---------------------\")\n",
        "        # -- training step\n",
        "        train()\n",
        "        # -- validation step\n",
        "        validate()"
      ],
      "metadata": {
        "id": "kIoexCysgS7q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}